serviceAccount:
  create: true
  annotations:
    eks.amazonaws.com/role-arn: ${sa_role_arn}
replicas: 2
revisionHistoryLimit: 10
strategy:
  rollingUpdate:
    maxUnavailable: 1
podDisruptionBudget:
  name: karpenter
  maxUnavailable: 1
podSecurityContext: 
  fsGroup: 65536
priorityClassName: system-cluster-critical
# -- Affinity rules for scheduling the pod. If an explicit label selector is not provided for pod affinity or pod anti-affinity one will be created from the pod selector labels.
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
            - key: karpenter.sh/nodepool
              operator: DoesNotExist
            - key: group
              operator: In
              values:
              - karpenter
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - topologyKey: "kubernetes.io/hostname"
# -- Topology spread constraints to increase the controller resilience by distributing pods across the cluster zones. If an explicit label selector is not provided one will be created from the pod selector labels.
topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
# -- Tolerations to allow the pod to be scheduled to nodes with taints.
tolerations:
  - key: CriticalAddonsOnly
    operator: Exists
  - key: "group"
    operator: "Equal"
    value: "karpenter"
    effect: "NoSchedule"
controller:
  image:
    repository: public.ecr.aws/karpenter/controller
  # resources:
  #   requests:
  #     cpu: 1
  #     memory: 1Gi
  #   limits:
  #     cpu: 1
  #     memory: 1Gi
  metrics:
    port: 8000
  healthProbe:
    port: 8081
# -- Global log level, defaults to 'info'
logLevel: debug
# -- Global Settings to configure Karpenter
settings:
  # -- The maximum length of a batch window. The longer this is, the more pods we can consider for provisioning at one
  # time which usually results in fewer but larger nodes.
  batchMaxDuration: 10s
  # -- The maximum amount of time with no new ending pods that if exceeded ends the current batching window. If pods arrive
  # faster than this time, the batching window will be extended up to the maxDuration. If they arrive slower, the pods
  # will be batched separately.
  batchIdleDuration: 1s
  # -- Role to assume for calling AWS services.
  assumeRoleARN: ""
  # -- Duration of assumed credentials in minutes. Default value is 15 minutes. Not used unless assumeRoleARN set.
  assumeRoleDuration: 15m
  # -- Cluster CA bundle for TLS configuration of provisioned nodes. If not set, this is taken from the controller's TLS configuration for the API server.
  clusterCABundle: ""
  # -- Cluster name.
  clusterName: "${cluster_name}"
  # -- Cluster endpoint. If not set, will be discovered during startup (EKS only)
  clusterEndpoint: ""
  # -- If true then assume we can't reach AWS services which don't have a VPC endpoint
  # This also has the effect of disabling look-ups to the AWS pricing endpoint
  isolatedVPC: false
  # -- The VM memory overhead as a percent that will be subtracted from the total memory for all instance types
  vmMemoryOverheadPercent: 0.075
  # -- interruptionQueue is disabled if not specified. Enabling interruption handling may
  # require additional permissions on the controller service account. Additional permissions are outlined in the docs.
  interruptionQueue: "${queue_name}"
  # -- Reserved ENIs are not included in the calculations for max-pods or kube-reserved
  # This is most often used in the VPC CNI custom networking setup https://docs.aws.amazon.com/eks/latest/userguide/cni-custom-network.html
  reservedENIs: "0"
  # -- Feature Gate configuration values. Feature Gates will follow the same graduation process and requirements as feature gates
  # in Kubernetes. More information here https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/#feature-gates-for-alpha-or-beta-features
  featureGates:
    # -- drift is in BETA and is enabled by default.
    # Setting drift to false disables the drift disruption method to watch for drift between currently deployed nodes
    # and the desired state of nodes set in nodepools and nodeclasses
    drift: true
    # -- spotToSpotConsolidation is disabled by default.
    # Setting this to true will enable spot replacement consolidation for both single and multi-node consolidation.
    spotToSpotConsolidation: false
